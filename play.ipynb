{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitbasecondaf53fe0a88a59447ca7ed8a320a0c5d56",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "import torch\n",
    "import numpy as np\n",
    "import imageio\n",
    "assert(trimesh.ray.has_embree)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'module' and 'int'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-0f64e209b61c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrimesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotation_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'module' and 'int'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<trimesh.Trimesh(vertices.shape=(298672, 3), faces.shape=(582206, 3))>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "target_mesh = trimesh.load(\"/home/jiahui/DR/DR/data/untitled.obj\")\n",
    "# target_mesh.vertices.shape\n",
    "target_mesh.apply_transform(trimesh.transformations.rotation_matrix(np.pi/2, [1,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scene will have automatically generated camera and lights\n",
    "scene = target_mesh.scene()\n",
    "# any of the automatically generated values can be overridden\n",
    "# set resolution, in pixels\n",
    "scene.camera.resolution = [100, 100]\n",
    "# set field of view, in degrees\n",
    "# make it relative to resolution so pixels per degree is same\n",
    "scene.camera.fov = (120,120)\n",
    "# convert the camera to rays with one ray per pixel\n",
    "origins, vectors, pixels = scene.camera_rays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 1.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  1.        ,  0.        ,  0.        ],\n       [-0.        ,  0.        ,  1.        ,  1.47058678],\n       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "scene = target_mesh.scene()\n",
    "scene.camera_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bunny = trimesh.load(\"../bunny.obj\")\n",
    "bunny.vertices -= bunny.center_mass\n",
    "bunny.apply_transform([[1,0,0,0],\n",
    "                        [0,0,1,0],\n",
    "                        [0,-1,0,0],\n",
    "                        [0,0,0,1]])\n",
    "# bak_vertices = bunny.vertices\n",
    "bak_vertices = torch.tensor(bunny.vertices,dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bunny_sm = trimesh.load(\"bunny_sm_sub.obj\")\n",
    "bunny_sm.vertices -= bunny_sm.center_mass\n",
    "bunny_sm.apply_transform([[1,0,0,0],\n",
    "                        [0,0,1,0],\n",
    "                        [0,-1,0,0],\n",
    "                        [0,0,0,1]])\n",
    "_=bunny_sm.export(\"start.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple camera\n",
    "x = torch.linspace(-0.2, 0.2, 256)\n",
    "z = torch.linspace(-0.2, 0.2, 256)\n",
    "xv,zv=torch.meshgrid(x,z)\n",
    "yv = torch.ones_like(xv)\n",
    "\n",
    "ray_dir = torch.stack((xv.reshape(-1), yv.reshape(-1), zv.reshape(-1)), dim=1)\n",
    "origin = torch.zeros_like(ray_dir) + torch.tensor([0,-0.5,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_norm(vertices, mesh):\n",
    "    ind_tri, ind_ray = mesh.ray.intersects_id(origin, ray_dir,False, 1, False)\n",
    "    ind_vert = mesh.faces[ind_tri]\n",
    "\n",
    "    # <<Fast, Minimum Storage Ray/Triangle Intersection>> \n",
    "    # https://cadxfem.org/inf/Fast%20MinimumStorage%20RayTriangle%20Intersection.pdf\n",
    "    def dot(v1, v2):\n",
    "        return v1[:,0]*v2[:,0] + v1[:,1]*v2[:,1] + v1[:,2]*v2[:,2]\n",
    "\n",
    "    v0 = vertices[ind_vert[:,0]]\n",
    "    v1 = vertices[ind_vert[:,1]]\n",
    "    v2 = vertices[ind_vert[:,2]]\n",
    "    n0 = torch.tensor(mesh.vertex_normals[ind_vert[:,0]], dtype=torch.float)\n",
    "    n1 = torch.tensor(mesh.vertex_normals[ind_vert[:,1]], dtype=torch.float)\n",
    "    n2 = torch.tensor(mesh.vertex_normals[ind_vert[:,2]], dtype=torch.float)\n",
    "\n",
    "    # Find vectors for two edges sharing v[0]\n",
    "    edge1 = v1-v0\n",
    "    edge2 = v2-v0\n",
    "    pvec = torch.cross(ray_dir[ind_ray], edge2)\n",
    "\n",
    "    # If determinant is near zero, ray lies in plane of triangle\n",
    "    det = dot(edge1, pvec)\n",
    "    inv_det = 1/det\n",
    "    # Calculate distance from v[0] to ray origin\n",
    "    tvec = origin[ind_ray] - v0\n",
    "    # Calculate U parameter\n",
    "    u = dot(tvec, pvec) * inv_det\n",
    "    qvec = torch.cross(tvec, edge1)\n",
    "    # Calculate V parameter\n",
    "    v = dot(ray_dir[ind_ray], qvec) * inv_det\n",
    "    # Calculate T\n",
    "    t = dot(edge2, qvec) * inv_det\n",
    "    # print( v.max(), v.min())\n",
    "    assert( v.max()<=1.00001 and v.min()>=-0.00001)\n",
    "    assert( u.max()<=1.00001 and u.min()>=-0.00001)\n",
    "    assert( (v+u).max()<=1.00001 and (v+u).min()>=-0.00001)\n",
    "    assert(t.min()>0)\n",
    "    # interpolate normal\n",
    "    n = (1-u-v).reshape((-1,1)) * n0 + u.reshape((-1,1)) * n1 + v.reshape((-1, 1)) * n2\n",
    "    n = n / n.norm(dim=1).reshape(-1,1)\n",
    "\n",
    "    img = torch.zeros(ray_dir.shape)\n",
    "    img[ind_ray] = n\n",
    "    img = img.reshape(256,256,3)\n",
    "\n",
    "    return img\n",
    "\n",
    "def render_facenorm(vertices, mesh):\n",
    "    ind_tri, ind_ray = mesh.ray.intersects_id(origin, ray_dir,False, 1, False)\n",
    "    ind_vert = mesh.faces[ind_tri]\n",
    "\n",
    "\n",
    "    def dot(v1, v2):\n",
    "        return v1[:,0]*v2[:,0] + v1[:,1]*v2[:,1] + v1[:,2]*v2[:,2]\n",
    "\n",
    "    v0 = vertices[ind_vert[:,0]]\n",
    "    v1 = vertices[ind_vert[:,1]]\n",
    "    v2 = vertices[ind_vert[:,2]]\n",
    "\n",
    "    # Find vectors for two edges sharing v[0]\n",
    "    edge1 = v1-v0\n",
    "    edge2 = v2-v0\n",
    "    n = torch.cross(edge1, edge2)\n",
    "    n = n / n.norm(dim=1).reshape(-1,1)\n",
    "\n",
    "    img = torch.zeros(ray_dir.shape)\n",
    "    img[ind_ray] = n\n",
    "    img = img.reshape(256,256,3)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Lossy conversion from float32 to uint8. Range [-0.9999842047691345, 0.999808669090271]. Convert image to uint8 prior to saving to suppress this warning.\n"
    }
   ],
   "source": [
    "# render target normal map\n",
    "bunny.vertices = bak_vertices\n",
    "vertices = torch.tensor(bunny.vertices, dtype=torch.float)\n",
    "\n",
    "# target = render_norm(vertices, bunny)\n",
    "target = render_norm(vertices, bunny)\n",
    "imageio.imsave(\"target.png\", target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [-0.02, 0.0, 0.02]:\n",
    "    for j in [-0.02, 0.0, 0.02]:\n",
    "        offset = [i, 0, j]\n",
    "        print(\"offset\", offset, \"gradient\", cal_gradient(offset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Iteration 000: error=0.365846\nIteration 001: error=0.351126\nIteration 002: error=0.337245\nIteration 003: error=0.320935\nIteration 004: error=0.297336\nIteration 005: error=0.262482\nIteration 006: error=0.215656\nIteration 007: error=0.159686\nIteration 008: error=0.0981118\nIteration 009: error=0.0463325\nIteration 010: error=0.0534497\nIteration 011: error=0.0726497\nIteration 012: error=0.0930754\nIteration 013: error=0.103609\nIteration 014: error=0.100506\nIteration 015: error=0.0879809\nIteration 016: error=0.0689747\nIteration 017: error=0.0482964\nIteration 018: error=0.0355741\nIteration 019: error=0.0347945\nIteration 020: error=0.04499\nIteration 021: error=0.0577449\nIteration 022: error=0.0645674\nIteration 023: error=0.0635588\nIteration 024: error=0.056293\nIteration 025: error=0.0437389\nIteration 026: error=0.029128\nIteration 027: error=0.0196685\nIteration 028: error=0.0203344\nIteration 029: error=0.0234401\n"
    }
   ],
   "source": [
    "off = [0.02, 0, 0.02] # move right and down\n",
    "parameter = torch.tensor(off, requires_grad=True)\n",
    "opt = torch.optim.Adam([parameter], lr=.002)\n",
    "gif = []\n",
    "for it in range(30):\n",
    "    # Zero out gradients before each iteration\n",
    "    opt.zero_grad()\n",
    "    vertices = bak_vertices + parameter.expand_as(bak_vertices)\n",
    "    bunny.vertices = vertices.detach().numpy()\n",
    "    img = render_norm(vertices, bunny)\n",
    "    # imageio.imsave(\"img{}.png\".format(off), img.detach())\n",
    "    loss = (img-target).pow(2).sum()/(256*256)\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimizer: take a gradient step\n",
    "    opt.step()\n",
    "\n",
    "    # imageio.imsave(\"img_{}.png\".format(it), img.detach())\n",
    "    print('Iteration %03i: error=%g' % (it, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bunny_sm = trimesh.load(\"bunny_sm_sub.obj\")\n",
    "bunny_sm.vertices -= bunny_sm.center_mass\n",
    "bunny_sm.apply_transform([[1,0,0,0],\n",
    "                        [0,0,1,0],\n",
    "                        [0,-1,0,0],\n",
    "                        [0,0,0,1]])\n",
    "sm_vertices = torch.tensor(bunny_sm.vertices,dtype=torch.float)\n",
    "sm_normals = torch.tensor(bunny_sm.vertex_normals, dtype=torch.float)\n",
    "\n",
    "# parameter = torch.zeros((bunny_sm.vertices.shape[0],1), requires_grad=True)\n",
    "parameter = torch.zeros(bunny_sm.vertices.shape, requires_grad=True)\n",
    "opt = torch.optim.Adam([parameter], lr=.00002)\n",
    "gif = []\n",
    "for it in range(90):\n",
    "    # Zero out gradients before each iteration\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # vertices = sm_vertices + parameter * sm_normals\n",
    "    vertices = sm_vertices + parameter\n",
    "    bunny_sm.vertices = vertices.detach().numpy()\n",
    "    img = render_facenorm(vertices, bunny_sm)\n",
    "    gif.append(img.detach().numpy())\n",
    "    loss = (img-target).pow(2).sum()/(256*256)\n",
    "    (loss + 0.1*parameter.pow(2).sum()).backward()\n",
    "\n",
    "    # Optimizer: take a gradient step\n",
    "    opt.step()\n",
    "\n",
    "    # imageio.imsave(\"result/img_{}.png\".format(it), img.detach())\n",
    "    print('Iteration %03i: error=%g' % (it, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm_vertices = torch.tensor(bunny_sm.vertices,dtype=torch.float)\n",
    "\n",
    "# parameter = torch.zeros((bunny_sm.vertices.shape[0],1), requires_grad=True)\n",
    "parameter = torch.zeros(bunny_sm.vertices.shape, requires_grad=True)\n",
    "opt = torch.optim.Adam([parameter], lr=.00005)\n",
    "for it in range(90):\n",
    "    # Zero out gradients before each iteration\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # vertices = sm_vertices + parameter * sm_normals\n",
    "    vertices = sm_vertices + parameter\n",
    "    bunny_sm.vertices = vertices.detach().numpy()\n",
    "    img = render_facenorm(vertices, bunny_sm)\n",
    "    loss = (img-target).pow(2).sum()/(256*256)\n",
    "    (loss + 0.1*parameter.pow(2).sum()).backward()\n",
    "\n",
    "    # Optimizer: take a gradient step\n",
    "    opt.step()\n",
    "\n",
    "    # imageio.imsave(\"result/img_{}.png\".format(it), img.detach())\n",
    "    print('Iteration %03i: error=%g' % (it, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Zero images were written.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f68939e6e82a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optim.gif\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgif\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/imageio/core/functions.py\u001b[0m in \u001b[0;36mmimwrite\u001b[0;34m(uri, ims, format, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;31m# be a generator. The damage is done, but we want to error when it happens.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwritten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Zero images were written.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;31m# Return a result if there is any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Zero images were written."
     ]
    }
   ],
   "source": [
    "imageio.mimsave(\"optim.gif\", gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}